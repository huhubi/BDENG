{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Test",
   "id": "7f9192c631596093"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T19:08:40.543375Z",
     "start_time": "2024-06-21T19:08:38.922587Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, expr\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"EmissionsAnalysis\").getOrCreate()\n",
    "\n",
    "# Define schema for the CSV file\n",
    "schema = StructType([\n",
    "    StructField(\"Year\", StringType(), True),\n",
    "    StructField(\"Emission\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Load the CSV data into a Spark DataFrame\n",
    "csv_file_path = 'emissions_2000_2022.csv'\n",
    "emissions_df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Reshape the DataFrame from wide to long format\n",
    "emissions_long_df = emissions_df.selectExpr(\"stack(23, '2000', `2000`, '2001', `2001`, '2002', `2002`, '2003', `2003`, '2004', `2004`, '2005', `2005`, '2006', `2006`, '2007', `2007`, '2008', `2008`, '2009', `2009`, '2010', `2010`, '2011', `2011`, '2012', `2012`, '2013', `2013`, '2014', `2014`, '2015', `2015`, '2016', `2016`, '2017', `2017`, '2018', `2018`, '2019', `2019`, '2020', `2020`, '2021', `2021`, '2022', `2022`) as (Year, Emission)\")\n",
    "\n",
    "# Display a sample of the reshaped DataFrame\n",
    "emissions_long_df.show(10)\n",
    "\n",
    "sample_data = emissions_long_df.limit(10)\n",
    "sample_data.show()\n",
    "sample_data_pd = sample_data.toPandas()\n",
    "print(sample_data_pd)"
   ],
   "id": "624072e46c7cd3df",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+\n",
      "|Year|        Emission|\n",
      "+----+----------------+\n",
      "|2000|7.30155652757513|\n",
      "|2001|7.34454985089927|\n",
      "|2002|7.59108201908797|\n",
      "|2003|7.66610658193886|\n",
      "|2004|7.92645513487823|\n",
      "|2005|8.02297170198171|\n",
      "|2006|8.24730138801443|\n",
      "|2007|8.35587998568037|\n",
      "|2008|8.47417247950314|\n",
      "|2009|8.47740613240567|\n",
      "+----+----------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+----+----------------+\n",
      "|Year|        Emission|\n",
      "+----+----------------+\n",
      "|2000|7.30155652757513|\n",
      "|2001|7.34454985089927|\n",
      "|2002|7.59108201908797|\n",
      "|2003|7.66610658193886|\n",
      "|2004|7.92645513487823|\n",
      "|2005|8.02297170198171|\n",
      "|2006|8.24730138801443|\n",
      "|2007|8.35587998568037|\n",
      "|2008|8.47417247950314|\n",
      "|2009|8.47740613240567|\n",
      "+----+----------------+\n",
      "\n",
      "   Year  Emission\n",
      "0  2000  7.301557\n",
      "1  2001  7.344550\n",
      "2  2002  7.591082\n",
      "3  2003  7.666107\n",
      "4  2004  7.926455\n",
      "5  2005  8.022972\n",
      "6  2006  8.247301\n",
      "7  2007  8.355880\n",
      "8  2008  8.474172\n",
      "9  2009  8.477406\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T19:14:57.800323Z",
     "start_time": "2024-06-21T19:14:57.755226Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['hadoop.home.dir'] = os.environ['HADOOP_HOME']\n",
    "os.environ['PATH'] += os.pathsep + os.path.join(os.environ['HADOOP_HOME'], 'bin')\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"EmissionsAnalysis\").enableHiveSupport().getOrCreate()\n",
    "\n",
    "# Load the CSV data into a Spark DataFrame\n",
    "csv_file_path = 'emissions_2000_2022.csv'  # Update this path to your actual file path\n",
    "emissions_df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Reshape the DataFrame from wide to long format\n",
    "emissions_long_df = emissions_df.selectExpr(\"stack(23, '2000', `2000`, '2001', `2001`, '2002', `2002`, '2003', `2003`, '2004', `2004`, '2005', `2005`, '2006', `2006`, '2007', `2007`, '2008', `2008`, '2009', `2009`, '2010', `2010`, '2011', `2011`, '2012', `2012`, '2013', `2013`, '2014', `2014`, '2015', `2015`, '2016', `2016`, '2017', `2017`, '2018', `2018`, '2019', `2019`, '2020', `2020`, '2021', `2021`, '2022', `2022`) as (Year, Emission)\")\n",
    "\n",
    "# Create or replace a temporary view\n",
    "emissions_long_df.createOrReplaceTempView(\"emissions_temp\")\n",
    "\n",
    "# Create a permanent table from the temporary view\n",
    "spark.sql(\"DROP TABLE IF EXISTS emission_data\")\n",
    "spark.sql(\"CREATE TABLE emission_data AS SELECT * FROM emissions_temp\")\n",
    "\n",
    "# Query the table\n",
    "result = spark.sql(\"SELECT * FROM emission_data\")\n",
    "result.show()"
   ],
   "id": "8c36be2716c495c5",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'HADOOP_HOME'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 5\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mos\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msys\u001B[39;00m\n\u001B[1;32m----> 5\u001B[0m os\u001B[38;5;241m.\u001B[39menviron[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhadoop.home.dir\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39menviron[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mHADOOP_HOME\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m      6\u001B[0m os\u001B[38;5;241m.\u001B[39menviron[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPATH\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpathsep \u001B[38;5;241m+\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(os\u001B[38;5;241m.\u001B[39menviron[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mHADOOP_HOME\u001B[39m\u001B[38;5;124m'\u001B[39m], \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbin\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m# Initialize Spark session\u001B[39;00m\n",
      "File \u001B[1;32m<frozen os>:679\u001B[0m, in \u001B[0;36m__getitem__\u001B[1;34m(self, key)\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: 'HADOOP_HOME'"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T19:49:50.906181Z",
     "start_time": "2024-06-21T19:49:50.878157Z"
    }
   },
   "cell_type": "code",
   "source": "spark.close()",
   "id": "d5769b236168acb2",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SparkSession' object has no attribute 'close'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m spark\u001B[38;5;241m.\u001B[39mclose()\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'SparkSession' object has no attribute 'close'"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3f38a4bb4734f1fe"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
