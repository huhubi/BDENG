{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Consumer",
   "id": "e3785fa8e04924a7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T12:00:38.956799Z",
     "start_time": "2024-06-25T12:00:38.862146Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from time import sleep\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Spark session with Kafka package\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaSparkStreaming\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define schema for the data\n",
    "schema = StructType([\n",
    "    StructField(\"year\", IntegerType(), True),\n",
    "    StructField(\"value\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame representing the stream of input lines from Kafka\n",
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"127.0.0.1:29092\") \\\n",
    "    .option(\"subscribe\", \"kfz_bestand\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# Cast the value column to STRING\n",
    "df = df.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "# Parse JSON data\n",
    "df = df.select(from_json(col(\"value\"), schema).alias(\"data\")).select(\"data.*\")\n",
    "\n",
    "# Write the streaming DataFrame to an in-memory table\n",
    "queryStream = df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"kfz_bestand_table\") \\\n",
    "    .start()\n",
    "\n",
    "# Initialize Seaborn\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rc('font', family='DejaVu Sans')\n",
    "\n",
    "# Wait for the streaming query to be ready\n",
    "sleep(10)  # Adjust the sleep time as needed to ensure the stream starts\n",
    "\n",
    "try:\n",
    "    i = 1\n",
    "    while True:\n",
    "        # Clear output\n",
    "        clear_output(wait=True)\n",
    "        print(\"**********************\")\n",
    "        print(\"General Info\")\n",
    "        print(\"**********************\")\n",
    "        print(\"Run:{}\".format(i))\n",
    "        if len(queryStream.recentProgress) > 0:\n",
    "            print(queryStream.lastProgress)\n",
    "            print(\"Stream timestamp:{}\".format(queryStream.lastProgress.get(\"timestamp\", \"N/A\")))\n",
    "            event_time = queryStream.lastProgress.get(\"eventTime\", {})\n",
    "            if \"watermark\" in event_time:\n",
    "                print(\"Watermark:{}\".format(event_time[\"watermark\"]))\n",
    "            state_operators = queryStream.lastProgress.get(\"stateOperators\", [])\n",
    "            if state_operators:\n",
    "                print(\"Total Rows:{}\".format(state_operators[0].get(\"numRowsTotal\", \"N/A\")))\n",
    "                print(\"Updated Rows:{}\".format(state_operators[0].get(\"numRowsUpdated\", \"N/A\")))\n",
    "                print(\"Memory used MB:{}\".format((state_operators[0].get(\"memoryUsedBytes\", 0)) * 0.000001))\n",
    "\n",
    "        # Fetch data from the in-memory table\n",
    "        df_pandas = spark.sql(\"SELECT * FROM kfz_bestand_table\").toPandas()\n",
    "\n",
    "        # Plot the data\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x='year', y='value', data=df_pandas)\n",
    "        plt.xlabel('Year')\n",
    "        plt.ylabel('Kfz-Bestand Value')\n",
    "        plt.title('Real-time Kfz-Bestand over Years')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Display DataFrame\n",
    "        print(\"**********************\")\n",
    "        print(\"Table - Kfz-Bestand Data\")\n",
    "        print(\"**********************\")\n",
    "        display(df_pandas)\n",
    "\n",
    "        # Sleep before the next update\n",
    "        sleep(3)\n",
    "        i += 1\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Process interrupted.\")\n",
    "finally:\n",
    "    queryStream.stop()\n",
    "    spark.stop()\n"
   ],
   "id": "c1ae2bc543a7404e",
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Failed to find data source: kafka. Please deploy the application as per the deployment section of Structured Streaming + Kafka Integration Guide.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 29\u001B[0m\n\u001B[0;32m     17\u001B[0m schema \u001B[38;5;241m=\u001B[39m StructType([\n\u001B[0;32m     18\u001B[0m     StructField(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myear\u001B[39m\u001B[38;5;124m\"\u001B[39m, IntegerType(), \u001B[38;5;28;01mTrue\u001B[39;00m),\n\u001B[0;32m     19\u001B[0m     StructField(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalue\u001B[39m\u001B[38;5;124m\"\u001B[39m, StringType(), \u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     20\u001B[0m ])\n\u001B[0;32m     22\u001B[0m \u001B[38;5;66;03m# Create DataFrame representing the stream of input lines from Kafka\u001B[39;00m\n\u001B[0;32m     23\u001B[0m df \u001B[38;5;241m=\u001B[39m spark \\\n\u001B[0;32m     24\u001B[0m     \u001B[38;5;241m.\u001B[39mreadStream \\\n\u001B[0;32m     25\u001B[0m     \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mkafka\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[0;32m     26\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mkafka.bootstrap.servers\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m127.0.0.1:29092\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[0;32m     27\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msubscribe\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mkfz_bestand\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[0;32m     28\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstartingOffsets\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mearliest\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m---> 29\u001B[0m     \u001B[38;5;241m.\u001B[39mload()\n\u001B[0;32m     31\u001B[0m \u001B[38;5;66;03m# Cast the value column to STRING\u001B[39;00m\n\u001B[0;32m     32\u001B[0m df \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mselectExpr(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCAST(value AS STRING)\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\streaming\\readwriter.py:304\u001B[0m, in \u001B[0;36mDataStreamReader.load\u001B[1;34m(self, path, format, schema, **options)\u001B[0m\n\u001B[0;32m    302\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_df(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jreader\u001B[38;5;241m.\u001B[39mload(path))\n\u001B[0;32m    303\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 304\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_df(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jreader\u001B[38;5;241m.\u001B[39mload())\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m   1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m-> 1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[0;32m   1323\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[0;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[0;32m   1326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[1;34m(*a, **kw)\u001B[0m\n\u001B[0;32m    181\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[0;32m    182\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[0;32m    183\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[0;32m    184\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[1;32m--> 185\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    186\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    187\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[1;31mAnalysisException\u001B[0m: Failed to find data source: kafka. Please deploy the application as per the deployment section of Structured Streaming + Kafka Integration Guide."
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T11:59:20.276367Z",
     "start_time": "2024-06-25T11:59:20.019374Z"
    }
   },
   "cell_type": "code",
   "source": "spark.stop()",
   "id": "bfd3a208c5c0ef9c",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "78df6147c25641ca"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
