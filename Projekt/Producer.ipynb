{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cea6f0962b0ecfa9",
   "metadata": {},
   "source": [
    "# Producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3cfbe23ffb7cd9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T12:06:41.771146Z",
     "start_time": "2024-06-25T12:06:03.895420Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22 rows\n",
      "Found year: 1990\n",
      "Found year: 1995\n",
      "Found year: 2000\n",
      "Year: 2000, Value: 4097145\n",
      "Found year: 2005\n",
      "Year: 2005, Value: 4156743\n",
      "Found year: 2006\n",
      "Year: 2006, Value: 4204969\n",
      "Found year: 2007\n",
      "Year: 2007, Value: 4245583\n",
      "Found year: 2008\n",
      "Year: 2008, Value: 4284919\n",
      "Found year: 2009\n",
      "Year: 2009, Value: 4359944\n",
      "Found year: 2010\n",
      "Year: 2010, Value: 4441027\n",
      "Found year: 2011\n",
      "Year: 2011, Value: 4513421\n",
      "Found year: 2012\n",
      "Year: 2012, Value: 4584202\n",
      "Found year: 2013\n",
      "Year: 2013, Value: 4641308\n",
      "Found year: 2014\n",
      "Year: 2014, Value: 4694921\n",
      "Found year: 2015\n",
      "Year: 2015, Value: 4748048\n",
      "Found year: 2016\n",
      "Year: 2016, Value: 4821557\n",
      "Found year: 2017\n",
      "Year: 2017, Value: 4898578\n",
      "Found year: 2018\n",
      "Year: 2018, Value: 4978852\n",
      "Found year: 2019\n",
      "Year: 2019, Value: 5039548\n",
      "Found year: 2020\n",
      "Year: 2020, Value: 5091827\n",
      "Found year: 2021\n",
      "Found year: 2022\n",
      "Found year: 2023\n",
      "{2000: '4097145', 2005: '4156743', 2006: '4204969', 2007: '4245583', 2008: '4284919', 2009: '4359944', 2010: '4441027', 2011: '4513421', 2012: '4584202', 2013: '4641308', 2014: '4694921', 2015: '4748048', 2016: '4821557', 2017: '4898578', 2018: '4978852', 2019: '5039548', 2020: '5091827'}\n",
      "Message delivered to kfz_bestand [0]\n",
      "Message delivered to kfz_bestand [0]\n",
      "Message delivered to kfz_bestand [0]\n",
      "Message delivered to kfz_bestand [0]\n",
      "Message delivered to kfz_bestand [0]\n",
      "Message delivered to kfz_bestand [0]\n",
      "Message delivered to kfz_bestand [0]\n",
      "Message delivered to kfz_bestand [0]\n",
      "Message delivered to kfz_bestand [0]\n",
      "Message delivered to kfz_bestand [0]\n",
      "Message delivered to kfz_bestand [0]\n",
      "Message delivered to kfz_bestand [0]\n",
      "Message delivered to kfz_bestand [0]\n",
      "Message delivered to kfz_bestand [0]\n",
      "Message delivered to kfz_bestand [0]\n",
      "Message delivered to kfz_bestand [0]\n",
      "Message delivered to kfz_bestand [0]\n",
      "Data sent to Kafka topic\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "from confluent_kafka import Producer\n",
    "import json\n",
    "import time\n",
    "\n",
    "# URL of the page to scrape\n",
    "url = \"https://www.statistik.at/statistiken/tourismus-und-verkehr/fahrzeuge/kfz-bestand\"\n",
    "\n",
    "# Initialize the WebDriver (make sure you have the ChromeDriver installed and in your PATH)\n",
    "options = Options()\n",
    "options.add_argument('--headless')  # Run in headless mode (no GUI)\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "# Fetch the page content\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the JavaScript to load content (adjust the time as necessary)\n",
    "time.sleep(10)\n",
    "\n",
    "# Get the page source and parse it with BeautifulSoup\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Initialize a dictionary to store the data\n",
    "data = {}\n",
    "\n",
    "# Find the table with the required heading\n",
    "table_heading = soup.find('span', class_='title-customized-padding', string='Kfz-Bestand 1990 bis 2023 (Tabelle)')\n",
    "if not table_heading:\n",
    "    print(\"Table heading not found.\")\n",
    "    exit()\n",
    "\n",
    "# The table should be the next sibling of the heading\n",
    "table = table_heading.find_next('table', class_='datatable')\n",
    "if not table:\n",
    "    print(\"Table not found.\")\n",
    "    exit()\n",
    "\n",
    "# Find all rows in the table\n",
    "rows = table.find_all(\"tr\", class_=[\"datatable__tr odd\", \"datatable__tr even\"])\n",
    "\n",
    "print(f\"Found {len(rows)} rows\")\n",
    "\n",
    "for row in rows:\n",
    "    # Extract the year\n",
    "    year_td = row.find(\"td\", class_=\"datatable__td dtr-control\")\n",
    "    if year_td:\n",
    "        year_text = year_td.get_text().strip()\n",
    "        print(f\"Found year: {year_text}\")  # Debug print\n",
    "        try:\n",
    "            year = int(year_text)\n",
    "            if year in range(2000, 2021):  # Interested in years 2000 to 2020\n",
    "                # Extract the second value for Personenkraftwagen column\n",
    "                values = row.find_all(\"td\", class_=\"datatable__td datatable__td--right\")\n",
    "                if values and len(values) >= 2:\n",
    "                    # Get the raw data for Personenkraftwagen\n",
    "                    raw_value = values[1].get_text().strip().replace('\\xa0', '')\n",
    "                    print(f\"Year: {year}, Value: {raw_value}\")  # Debug print\n",
    "                    data[year] = raw_value\n",
    "        except ValueError:\n",
    "            continue  # Skip rows where the year is not a valid integer\n",
    "\n",
    "print(data)\n",
    "\n",
    "# Kafka configuration\n",
    "conf = {\n",
    "    'bootstrap.servers': '127.0.0.1:29092'\n",
    "}\n",
    "\n",
    "# Create Producer instance\n",
    "producer = Producer(conf)\n",
    "\n",
    "# Kafka topic\n",
    "topic = 'kfz_bestand'\n",
    "\n",
    "# Produce the JSON data to Kafka topic\n",
    "def delivery_report(err, msg):\n",
    "    if err is not None:\n",
    "        print(f\"Message delivery failed: {err}\")\n",
    "    else:\n",
    "        print(f\"Message delivered to {msg.topic()} [{msg.partition()}]\")\n",
    "\n",
    "# Send each year data in a loop with a one-second interval\n",
    "for year, value in data.items():\n",
    "    json_data = json.dumps({year: value})\n",
    "    producer.produce(topic, value=json_data, callback=delivery_report)\n",
    "    producer.flush()\n",
    "    time.sleep(1)\n",
    "\n",
    "print(\"Data sent to Kafka topic\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "feb1fdc9ceaf683b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T11:59:14.880395Z",
     "start_time": "2024-06-25T11:59:14.104624Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Stop the Spark session\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m spark\u001b[38;5;241m.\u001b[39mstop()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# Stop the Spark session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb40893e27c7e9f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T12:06:57.167368Z",
     "start_time": "2024-06-25T12:06:41.771146Z"
    }
   },
   "outputs": [
    {
     "ename": "PySparkRuntimeError",
     "evalue": "[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPySparkRuntimeError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 15\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Initialize Spark session with Kafka package\u001b[39;00m\n\u001b[0;32m     12\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder \\\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKafkaSparkStreaming\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.jars.packages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m---> 15\u001b[0m     \u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Define schema for the data\u001b[39;00m\n\u001b[0;32m     18\u001b[0m schema \u001b[38;5;241m=\u001b[39m StructType([\n\u001b[0;32m     19\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m\"\u001b[39m, IntegerType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m     20\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     21\u001b[0m ])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\session.py:497\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    495\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[0;32m    496\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 497\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39mgetOrCreate(sparkConf)\n\u001b[0;32m    498\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[0;32m    500\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\context.py:515\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 515\u001b[0m         SparkContext(conf\u001b[38;5;241m=\u001b[39mconf \u001b[38;5;129;01mor\u001b[39;00m SparkConf())\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\context.py:201\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    199\u001b[0m     )\n\u001b[1;32m--> 201\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[0;32m    204\u001b[0m         master,\n\u001b[0;32m    205\u001b[0m         appName,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[0;32m    216\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\context.py:436\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[1;32m--> 436\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m launch_gateway(conf)\n\u001b[0;32m    437\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[0;32m    439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\java_gateway.py:107\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    104\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[1;32m--> 107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[0;32m    108\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJAVA_GATEWAY_EXITED\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    109\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[0;32m    110\u001b[0m     )\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[0;32m    113\u001b[0m     gateway_port \u001b[38;5;241m=\u001b[39m read_int(info)\n",
      "\u001b[1;31mPySparkRuntimeError\u001b[0m: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number."
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from time import sleep\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Spark session with Kafka package\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaSparkStreaming\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define schema for the data\n",
    "schema = StructType([\n",
    "    StructField(\"year\", IntegerType(), True),\n",
    "    StructField(\"value\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame representing the stream of input lines from Kafka\n",
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"127.0.0.1:29092\") \\\n",
    "    .option(\"subscribe\", \"kfz_bestand\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# Cast the value column to STRING\n",
    "df = df.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "# Parse JSON data\n",
    "df = df.select(from_json(col(\"value\"), schema).alias(\"data\")).select(\"data.*\")\n",
    "\n",
    "# Write the streaming DataFrame to an in-memory table\n",
    "queryStream = df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"kfz_bestand_table\") \\\n",
    "    .start()\n",
    "\n",
    "# Initialize Seaborn\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rc('font', family='DejaVu Sans')\n",
    "\n",
    "# Wait for the streaming query to be ready\n",
    "sleep(10)  # Adjust the sleep time as needed to ensure the stream starts\n",
    "\n",
    "try:\n",
    "    i = 1\n",
    "    while True:\n",
    "        # Clear output\n",
    "        clear_output(wait=True)\n",
    "        print(\"**********************\")\n",
    "        print(\"General Info\")\n",
    "        print(\"**********************\")\n",
    "        print(\"Run:{}\".format(i))\n",
    "        if len(queryStream.recentProgress) > 0:\n",
    "            print(queryStream.lastProgress)\n",
    "            print(\"Stream timestamp:{}\".format(queryStream.lastProgress.get(\"timestamp\", \"N/A\")))\n",
    "            event_time = queryStream.lastProgress.get(\"eventTime\", {})\n",
    "            if \"watermark\" in event_time:\n",
    "                print(\"Watermark:{}\".format(event_time[\"watermark\"]))\n",
    "            state_operators = queryStream.lastProgress.get(\"stateOperators\", [])\n",
    "            if state_operators:\n",
    "                print(\"Total Rows:{}\".format(state_operators[0].get(\"numRowsTotal\", \"N/A\")))\n",
    "                print(\"Updated Rows:{}\".format(state_operators[0].get(\"numRowsUpdated\", \"N/A\")))\n",
    "                print(\"Memory used MB:{}\".format((state_operators[0].get(\"memoryUsedBytes\", 0)) * 0.000001))\n",
    "\n",
    "        # Fetch data from the in-memory table\n",
    "        df_pandas = spark.sql(\"SELECT * FROM kfz_bestand_table\").toPandas()\n",
    "\n",
    "        # Plot the data\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x='year', y='value', data=df_pandas)\n",
    "        plt.xlabel('Year')\n",
    "        plt.ylabel('Kfz-Bestand Value')\n",
    "        plt.title('Real-time Kfz-Bestand over Years')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Display DataFrame\n",
    "        print(\"**********************\")\n",
    "        print(\"Table - Kfz-Bestand Data\")\n",
    "        print(\"**********************\")\n",
    "        display(df_pandas)\n",
    "\n",
    "        # Sleep before the next update\n",
    "        sleep(3)\n",
    "        i += 1\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Process interrupted.\")\n",
    "finally:\n",
    "    queryStream.stop()\n",
    "    spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec2a209024e1bcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
